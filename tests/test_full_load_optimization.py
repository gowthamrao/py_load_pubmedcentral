import io
from pathlib import Path
import tarfile
from typer.testing import CliRunner
import pytest
from unittest.mock import MagicMock
from contextlib import contextmanager

from py_load_pubmedcentral.cli import app
from py_load_pubmedcentral.db import PostgreSQLAdapter
from py_load_pubmedcentral.models import ArticleFileInfo

# This test requires a database connection and an initialized schema.
pytestmark = [
    pytest.mark.usefixtures("test_db_adapter"),
    pytest.mark.integration,
]


def create_dummy_tar_gz(tar_path: Path, xml_files: dict[str, str]):
    """Helper to create a tar.gz file with specified XML content."""
    with tarfile.open(tar_path, "w:gz") as tar:
        for filename, content in xml_files.items():
            xml_bytes = content.encode("utf-8")
            tarinfo = tarfile.TarInfo(name=filename)
            tarinfo.size = len(xml_bytes)
            tar.addfile(tarinfo, io.BytesIO(xml_bytes))


@pytest.fixture
def mock_data_source(mocker, tmp_path):
    """
    Mocks the data source to prevent real downloads and provide
    controlled test data (two archives with one article each).
    """
    # Create dummy XML content
    xml_content_1 = '''<article>
        <front>
            <journal-meta><journal-title-group><journal-title>Test Journal</journal-title></journal-title-group></journal-meta>
            <article-meta>
                <article-id pub-id-type="pmc">PMC001</article-id>
                <title-group><article-title>Title 1</article-title></title-group>
                <contrib-group><contrib contrib-type="author"><name><surname>Author</surname><given-names>One</given-names></name></contrib></contrib-group>
            </article-meta>
        </front>
        <body><p>Test article 1.</p></body>
    </article>'''
    xml_content_2 = '''<article>
        <front>
            <journal-meta><journal-title-group><journal-title>Test Journal</journal-title></journal-title-group></journal-meta>
            <article-meta>
                <article-id pub-id-type="pmc">PMC002</article-id>
                <title-group><article-title>Title 2</article-title></title-group>
                <contrib-group><contrib contrib-type="author"><name><surname>Author</surname><given-names>Two</given-names></name></contrib></contrib-group>
            </article-meta>
        </front>
        <body><p>Test article 2.</p></body>
    </article>'''

    # Create dummy tar.gz files in a temporary directory
    archive1_path = tmp_path / "archive1.tar.gz"
    archive2_path = tmp_path / "archive2.tar.gz"
    create_dummy_tar_gz(archive1_path, {"PMC001.xml": xml_content_1})
    create_dummy_tar_gz(archive2_path, {"PMC002.xml": xml_content_2})

    def mock_download_worker(file_identifier, source_name, tmp_path_arg):
        if "archive1" in file_identifier:
            return archive1_path
        if "archive2" in file_identifier:
            return archive2_path
        return None

    mocker.patch("py_load_pubmedcentral.cli._download_archive_worker", side_effect=mock_download_worker)

    mock_source_instance = MagicMock()
    mock_article_file_list = [
        ArticleFileInfo(file_path="archive1.tar.gz", pmcid="PMC001", is_retracted=False),
        ArticleFileInfo(file_path="archive2.tar.gz", pmcid="PMC002", is_retracted=False),
    ]
    mock_source_instance.get_article_file_list.return_value = mock_article_file_list
    mocker.patch("py_load_pubmedcentral.cli.S3DataSource", return_value=mock_source_instance)
    mocker.patch("py_load_pubmedcentral.cli.NcbiFtpDataSource", return_value=mock_source_instance)


def test_full_load_uses_optimized_path(
    mocker,
    mock_data_source,
    test_db_adapter,
    tmp_path,
):
    """
    This test verifies the new, optimized behavior of `full_load`.

    It spies on the `bulk_insert_from_files_for_full_load` method and asserts
    that it's called exactly once with a list of all the intermediate TSV files
    generated by the parsing workers.
    """
    runner = CliRunner()

    # Mock tempfile.TemporaryDirectory to use our persistent tmp_path fixture
    @contextmanager
    def mock_tmpdir():
        yield tmp_path
    mocker.patch("tempfile.TemporaryDirectory", new=mock_tmpdir)

    # Initialize the database schema first
    schema_path = Path(__file__).parent.parent / "schemas" / "pmc_schema.sql"
    init_result = runner.invoke(app, ["initialize", "--schema", str(schema_path)])
    assert init_result.exit_code == 0

    # Spy on the NEW method that performs the final database load.
    spy_bulk_insert = mocker.spy(PostgreSQLAdapter, "bulk_insert_from_files_for_full_load")

    result = runner.invoke(
        app,
        [
            "full-load",
            "--source", "s3",
            "--parsing-workers", "2", # Use 2 workers to get 2 sets of TSV files
        ],
        catch_exceptions=False
    )

    assert result.exit_code == 0
    assert "Database loading complete" in result.stdout

    # 1. Assert that the new bulk load method was called exactly once.
    assert spy_bulk_insert.call_count == 1

    # 2. Inspect the arguments to confirm it was called with lists of files.
    call_args, call_kwargs = spy_bulk_insert.call_args
    metadata_file_paths = call_kwargs['metadata_file_paths']
    content_file_paths = call_kwargs['content_file_paths']

    # We used 2 workers, so we should have 2 intermediate files for each type.
    assert len(metadata_file_paths) == 2
    assert len(content_file_paths) == 2
    assert "metadata_" in metadata_file_paths[0]
    assert ".tsv" in metadata_file_paths[0]

    # 3. Verify that the data was actually loaded into the database.
    db_connection = test_db_adapter.conn
    with db_connection.cursor() as cursor:
        cursor.execute("SELECT COUNT(*) FROM pmc_articles_metadata WHERE pmcid IN ('PMC001', 'PMC002');")
        count = cursor.fetchone()[0]
        assert count == 2
